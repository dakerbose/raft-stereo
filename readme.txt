***cnet***
'''
RAFTStereo                                         [1, 1, 640, 960]          --
├─MultiBasicEncoder: 1-1                           [1, 128, 160, 240]        --
│    └─Conv2d: 2-1                                 [1, 64, 640, 960]         9,472 = 输入通道数3 * 输出通道数64 * 卷积核大小7 * 卷积核大小7 + 偏置项64
│    └─BatchNorm2d: 2-2                            [1, 64, 640, 960]         128 = 缩放因子64 + 偏置项64
│    └─ReLU: 2-3                                   [1, 64, 640, 960]         --
│    └─Sequential: 2-4                             [1, 64, 640, 960]         --
│    │    └─ResidualBlock: 3-1                     [1, 64, 640, 960]         74,112 = 2 * 卷积层36928 (输入通道64 * 输出通道数64 * 卷积核大小3 * 3 + 偏置项64) + 2 * 归一化层128
│    │    └─ResidualBlock: 3-2                     [1, 64, 640, 960]         74,112 同3-1
│    └─Sequential: 2-5                             [1, 96, 320, 480]         --
│    │    └─ResidualBlock: 3-3                     [1, 96, 320, 480]         145,248 = 两层卷积层 (输入通道[64+96] * 输出通道数96 * 卷积核大小3 * 3 + 偏置项96*2) + 3 * 归一化层(偏置项96+缩放因子96)+残差降采样 (输入通道数64*输出通道数96*卷积核大小1*1+偏置96)
│    │    └─ResidualBlock: 3-4                     [1, 96, 320, 480]         166,464 = 两层卷积层 (输入通道[96+96] * 输出通道数96 * 卷积核大小3 * 3 + 偏置项96*2) + 2 * 归一化层192
│    └─Sequential: 2-6                             [1, 128, 160, 240]        --
│    │    └─ResidualBlock: 3-5                     [1, 128, 160, 240]        271,488 = 两层卷积层 (输入通道[96+128] * 输出通道数128 * 卷积核大小3 * 3 + 偏置项128*2) + 3 * 归一化层(偏置项128+缩放因子128)+残差降采样 (输入通道数96*输出通道数128*卷积核大小1*1+偏置128)
│    │    └─ResidualBlock: 3-6                     [1, 128, 160, 240]        295,680 = 两层卷积层 (输入通道[128+128] * 输出通道数128 * 卷积核大小3 * 3 + 偏置项128*2) + 2 * 归一化层256
│    └─ModuleList: 2-7                             --                        --
│    │    └─Sequential: 3-7                        [1, 128, 160, 240]        443,264 = 和3-6一样的残差块295,680 + 一个卷积 128*128*9*128+卷积的偏置128
│    │    └─Sequential: 3-8                        [1, 128, 160, 240]        443,264 同3-7
│    └─Sequential: 2-8                             [1, 128, 80, 120]         --
│    │    └─ResidualBlock: 3-9                     [1, 128, 80, 120]         312,448 = 和3-6一样的残差块295680 + 降采样层(128 * 128 * 1 * 1 + 偏置128) + 归一化层(偏置项128+缩放因子128)
│    │    └─ResidualBlock: 3-10                    [1, 128, 80, 120]         295,680 同3-6
│    └─ModuleList: 2-9                             --                        --
│    │    └─Sequential: 3-11                       [1, 128, 80, 120]         443,264 同3-7
│    │    └─Sequential: 3-12                       [1, 128, 80, 120]         443,264 同3-7
│    └─Sequential: 2-10                            [1, 128, 40, 60]          --
│    │    └─ResidualBlock: 3-13                    [1, 128, 40, 60]          312,448 同3-9
│    │    └─ResidualBlock: 3-14                    [1, 128, 40, 60]          295,680 同3-6
│    └─ModuleList: 2-11                            --                        --
│    │    └─Conv2d: 3-15                           [1, 128, 40, 60]          147,584 输入通道数128 * 输出通道数128*9 + 偏置128
│    │    └─Conv2d: 3-16                           [1, 128, 40, 60]          147,584 同3-15
'''
ModuleList用于不同分辨率输出，context_dims和hidden_dims都是[128, 128, 128]，也就是说output_dim = [[128, 128, 128], [128, 128, 128]]
也就是说Sequential: 2-6里面有两层ResidualBlock，而ModuleList里面也有也有一层ResidualBlock 
'''
        output_list = []
        for dim in output_dim:
            conv_out = nn.Sequential(
                ResidualBlock(128, 128, self.norm_fn, stride=1),
                nn.Conv2d(128, dim[2], 3, padding=1))
            output_list.append(conv_out)

        self.outputs08 = nn.ModuleList(output_list)
'''
***
fnet
***
BasicEncoder的很多模块和MultiBasicEncoder类似，区别是归一化的部分换了所以省略这部分参数
‘‘‘
├─BasicEncoder: 1-2                                [1, 256, 160, 240]        --
│    └─Conv2d: 2-12                                [2, 64, 640, 960]         9,472 = 3 * 64 * 7 * 7 + 64 同2-1
│    └─InstanceNorm2d: 2-13                        [2, 64, 640, 960]         --
│    └─ReLU: 2-14                                  [2, 64, 640, 960]         --
│    └─Sequential: 2-15                            [2, 64, 640, 960]         --
│    │    └─ResidualBlock: 3-17                    [2, 64, 640, 960]         73,856 = (64 * 64 * 3 * 3 + 64) * 2 (因为是InstanceNorm所以没有计算量)
│    │    └─ResidualBlock: 3-18                    [2, 64, 640, 960]         73,856 同3-17
│    └─Sequential: 2-16                            [2, 96, 320, 480]         --
│    │    └─ResidualBlock: 3-19                    [2, 96, 320, 480]         144,672 = 两层卷积层 (输入通道[64+96] * 输出通道数96 * 卷积核大小3 * 3 + 偏置项96*2) + 残差降采样 (输入通道数64*输出通道数96*卷积核大小1*1+偏置96) 相比于3-3少了归一化的参数量
│    │    └─ResidualBlock: 3-20                    [2, 96, 320, 480]         166,080 = 两层卷积层 (输入通道[96+96] * 输出通道数96 * 卷积核大小3 * 3 + 偏置项96*2
│    └─Sequential: 2-17                            [2, 128, 160, 240]        --
│    │    └─ResidualBlock: 3-21                    [2, 128, 160, 240]        270,720 = 两层卷积层 (输入通道[96+128] * 输出通道数128 * 卷积核大小3 * 3 + 偏置项128*2) + 残差降采样 (输入通道数96*输出通道数128*卷积核大小1*1+偏置128)
│    │    └─ResidualBlock: 3-22                    [2, 128, 160, 240]        295,168 = 两层卷积层 (输入通道[128+128] * 输出通道数128 * 卷积核大小3 * 3 + 偏置项128*2)
│    └─Conv2d: 2-18                                [2, 256, 160, 240]        33,024  = 256 * 128 + 256
’’’


net_list 用于初始化 GRU 模块的隐藏状态
inp_list 作为上下文信息特征
‘‘‘
├─ModuleList: 1-3                                  --                        --
│    └─Conv2d: 2-19                                [1, 384, 160, 240]        442,752 = 输入通道数128 * 输出通道数384 * 卷积核大小3*3 + 偏置项384
│    └─Conv2d: 2-20                                [1, 384, 80, 120]         442,752 同2-20
│    └─Conv2d: 2-21                                [1, 384, 40, 60]          442,752 同2-21
‘‘‘


接下来就是重磅内容GRU，涉及到相关性代价金字塔
‘‘‘
├─BasicMultiUpdateBlock: 1-4                       [1, 128, 160, 240]        --
│    └─ConvGRU: 2-22                               [1, 128, 40, 60]          --      # self.gru32 = ConvGRU(hidden_dims[0], hidden_dims[1])
│    │    └─Conv2d: 3-23                           [1, 128, 40, 60]          295,040 = 128 * (128 + 128) * 3 * 3 + 128
│    │    └─Conv2d: 3-24                           [1, 128, 40, 60]          295,040 同3-23 
│    │    └─Conv2d: 3-25                           [1, 128, 40, 60]          295,040 同3-23
│    └─ConvGRU: 2-23                               [1, 128, 80, 120]         --      # self.gru16 = ConvGRU(hidden_dims[1], hidden_dims[0] * (args.n_gru_layers == 3) + hidden_dims[2])    
│    │    └─Conv2d: 3-26                           [1, 128, 80, 120]         442,496 = 隐藏层128 * (隐藏层128+输入层256) * 9 + 输出通道数隐藏层128
│    │    └─Conv2d: 3-27                           [1, 128, 80, 120]         442,496 同3-26
│    │    └─Conv2d: 3-28                           [1, 128, 80, 120]         442,496 同3-26
│    └─BasicMotionEncoder: 2-24                    [1, 128, 160, 240]        --
│    │    └─Conv2d: 3-29                           [1, 64, 160, 240]         2,368  = 4 * (4 * 2 + 1) * 64 * 1 * 1 + 64
│    │    └─Conv2d: 3-30                           [1, 64, 160, 240]         36,928 = 64 * 64 * 9 + 64
│    │    └─Conv2d: 3-31                           [1, 64, 160, 240]         6,336  = 2 * 64 * 7 * 7 + 64
│    │    └─Conv2d: 3-32                           [1, 64, 160, 240]         36,928 同3-30
│    │    └─Conv2d: 3-33                           [1, 126, 160, 240]        145,278 = 128*126*9+126  输出通道数减2是为了和两层flow的输入做拼接
│    └─ConvGRU: 2-25                               [1, 128, 160, 240]        -- self.gru08(net[0], *(inp[0]), motion_features, interp(net[1], net[0]))
│    │    └─Conv2d: 3-34                           [1, 128, 160, 240]        442,496 同3-26
│    │    └─Conv2d: 3-35                           [1, 128, 160, 240]        442,496 同3-26
│    │    └─Conv2d: 3-36                           [1, 128, 160, 240]        442,496 同3-26
│    └─FlowHead: 2-26                              [1, 2, 160, 240]          -- 只有在gru8的时候才进行更新操作，也就是输出得到up_mask, delta_flow
│    │    └─Conv2d: 3-37                           [1, 256, 160, 240]        295,168 = 128 * 256 * 9 + 256
│    │    └─ReLU: 3-38                             [1, 256, 160, 240]        --
│    │    └─Conv2d: 3-39                           [1, 2, 160, 240]          4,610 = 256 * 输出层2 * 9 + 2
│    └─Sequential: 2-27                            [1, 144, 160, 240]        -- mask在上采样过程中提供加权信息，从而增强光流估计的质量
│    │    └─Conv2d: 3-40                           [1, 256, 160, 240]        295,168 两层卷积层 (输入通道128 * 输出通道数256 * 卷积核大小3 * 3 + 偏置项128*2)
│    │    └─ReLU: 3-41                             [1, 256, 160, 240]        --
│    │    └─Conv2d: 3-42                           [1, 144, 160, 240]        37,008 = 256 * (16*9) + 144
’’’
关于BasicMotionEncoder的说明
flow是一个含光流信息的张量(batch_size, 2, height, width)，这个2的意思是表示垂直和水平两个方向
corr是相关性张量，通过计算光流和前一帧特征图之间的相关性得到的
convc1 和 convc2 处理相关性特征图 corr，将其转换为64个通道的特征图。
convf1 和 convf2 处理光流 flow，同样将其转换为64个通道的特征图。
最后，conv 将合并后的特征图（相关性特征图和光流特征图）映射到128-2个通道。
最终返回的结果是通过 torch.cat([out, flow], dim=1) 拼接了处理后的特征图 out 和原始的光流 flow
